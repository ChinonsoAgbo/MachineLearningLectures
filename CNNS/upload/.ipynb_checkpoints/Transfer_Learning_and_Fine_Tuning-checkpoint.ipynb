{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5439fa2b-db84-4534-8e75-3d6558bb1d0a",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine Tuning\n",
    "See https://keras.io/guides/transfer_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35e7e0-e242-40c8-81c2-d1f4b91d95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3b129-6579-4eeb-a8a7-40e084e1561e",
   "metadata": {},
   "source": [
    "## The Cats vs Dogs Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64144318-c798-4f4e-b288-b9d1abfae675",
   "metadata": {},
   "source": [
    "In this example a CNN classifier pre trained on ImageNet data will be transfer learned to a binary classifier and fine tuned, distinguishing between cats and dogs. As can be seen below. a few hundred images already suffice to reach high accuracies using this technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b7a67-c12b-4ae9-90ba-795da392f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()\n",
    "\n",
    "train_ds, validation_ds, test_ds = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    split=[\"train[:2%]\", \"train[40%:42%]\", \"train[50%:52%]\"],\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
    "print(\n",
    "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
    ")\n",
    "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f9fc2-feaf-4c8b-8385-7bd16ec6653d",
   "metadata": {},
   "source": [
    "#### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef4ab1-3990-43a0-ae1d-bda32812af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(int(label))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8028756-fff3-4c22-ae5e-f3e06381cd01",
   "metadata": {},
   "source": [
    "#### Setting up the preprocessing pipe line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee9d97-a97d-4e43-8b51-8073e8556763",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (224, 224)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b0b65-30e0-4192-9b57-956d35075d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a3b8a-f404-48fb-930b-3f57d2fc5333",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef963ad8-2412-4f96-938f-315088c7b5c4",
   "metadata": {},
   "source": [
    "Another popular method to avoid overfitting on little data is data augmentation. Images are randomly transformed, using selected operations, such that no images passes the training twice exaclty the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5dd670-3bc7-413f-bed8-e862ae56bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "    ]\n",
    ")\n",
    "for images, labels in train_ds.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = images[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(\n",
    "            tf.expand_dims(first_image, 0), training=True\n",
    "        )\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
    "        plt.title(int(labels[0]))\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8e6b2-4d89-4a47-a677-c994504abf4b",
   "metadata": {},
   "source": [
    "## Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410586e1-9aae-4302-aad7-0ad8c0268556",
   "metadata": {},
   "source": [
    "Many well published pretrained models can be found for keras on https://keras.io/api/applications/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b5f43-fc82-4a23-a96e-51afd932962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.MobileNet(\n",
    "    # Load weights pre-trained on ImageNet.\n",
    "    weights=\"imagenet\",  \n",
    "    input_shape=(224, 224, 3),\n",
    "    # Do not include the ImageNet classifier at the top.\n",
    "    include_top=False,\n",
    ") \n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "# As can be seen in the summary below, none of the models weights will be adapted during training.\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690fbc0-032b-4337-b9a7-5208b2e21968",
   "metadata": {},
   "source": [
    "We add the preprocessing and the scaling to the model and add a new Dense Layer on top of the pretrained model to adjust the feature extractor to the new taks during the transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120b2d9-c806-433c-90fe-98ad98764e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be scaled\n",
    "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
    "# outputs: `(inputs * scale) + offset`\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "# TODO: Add a GlobalAveragePooling, a Dropout and a Dense layer to the base model\n",
    "outputs = x\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3dd73a-76e6-47bc-9a0e-cbc3326cad1b",
   "metadata": {},
   "source": [
    "Training only the few parameters in the newly added top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2126110-25f4-4795-b63b-5c30b747dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9d790-21cf-45ae-98e6-5c1cc60a0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "# we've done so far.\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a45c7a-d8e4-4941-b43e-a72266480914",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928efdcb-eb6a-4568-a3f9-ed3bf061e682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
