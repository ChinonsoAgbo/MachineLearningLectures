{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1855d776-ce52-45ec-b794-8581fcc21afd",
   "metadata": {},
   "source": [
    "# First KNN with Keras"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f4a8baf-1a0b-4cd8-9f88-4d943f3d7f24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:25:14.146836Z",
     "start_time": "2024-05-27T10:25:14.022017Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "3f812785-29fc-4cd1-b6a2-9d745a76bf83",
   "metadata": {},
   "source": [
    "We'll use the MNIST dataset of handwritten digits as our first example of applying an artificial neural network in Keras. keras.datasets provides several benchmark data sets such this one. We'll split into a train set of 59 000 images, a validation set of 1000 images, and a test set of 10 000 Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45fca7c-9a97-4d7b-92e1-443f6d03afb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape (59000, 28, 28)\n",
      "Train Label Shape  (59000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_width = x_train.shape[1]\n",
    "x_height = x_train.shape[2]\n",
    "np.random.seed(2)\n",
    "\n",
    "x_val = x_train[:999,...]\n",
    "y_val = y_train[:999,...]\n",
    "\n",
    "x_train = x_train[1000:,...]\n",
    "y_train = y_train[1000:,...]\n",
    "\n",
    "print(\"Train Data Shape\", x_train.shape)\n",
    "print(\"Train Label Shape \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e04e82-5e23-4c0c-a9c3-8541ff858267",
   "metadata": {},
   "source": [
    "The digits are represented as grey value images of 28x28 pixels as depicted below. Each pixel intensity is represented as 8 bit number ranging from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43770fae-1b6a-4472-ab71-0ee8eaf1bab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_train[1],cmap=\"gist_gray\")\n",
    "plt.colorbar()\n",
    "plt.savefig(\"seven.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813cc024-a6f1-41c8-a96a-21d19cf57841",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6094cf1-b01e-47cd-8d57-f5bf5f3e2698",
   "metadata": {},
   "source": [
    "### One-Hot Ecnoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a93a26-0b65-4203-816d-75550a3a8b44",
   "metadata": {},
   "source": [
    "The MNIST data set contains digits from 0 to 9 and hence has 10 classes. The label for each class is represented as an according integer from 0 to 9. In our first artificial neural network, each class is represented by an output neuron, yielding a one score for each class. In order to compare the output to the labels, the labels need to be one-hot encoded. This means for each label entry, an array is generated with an length equal to the number of classes. Each entry is zero but the one corresponding to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccde3036-99ee-4237-b3ef-98ff3b50ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label representation as integer: 7\n",
      "Label representation as one-hot tensor: tf.Tensor([0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Label representation as integer:\", y_train[1])\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "one_hot_tensor = tf.one_hot(y_train[1], depth=n_classes)\n",
    "print(\"Label representation as one-hot tensor:\", one_hot_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aaca69-9784-494c-b7d3-4431cd427596",
   "metadata": {},
   "source": [
    "The one-hot encoding is one of several steps in the data pipeline. Data pipelines can be implemented using the tf.data module. In the following steps several preprocessing functions are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22f18de-3015-493f-901b-08c83ac0689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, the function used here are wrapped as tf.function objects using the respective decorator\n",
    "@tf.function\n",
    "def normalize_img(data, label):\n",
    "    # it is essential to scale the data. Since each pixel is represented by a 8 bit integer, we'll divide by 255 to normalize the pixel values to [0,1]\n",
    "    return tf.cast(data, tf.float32) / 255., label\n",
    "    \n",
    "@tf.function\n",
    "def one_hot_encoding(data, label):\n",
    "    # Performing the one-hot encoding using the tesnorflow function demonstrated above\n",
    "    one_hot_labels = tf.one_hot(label, depth=n_classes)\n",
    "    return data, one_hot_labels\n",
    "\n",
    "# defining the training data pipe line\n",
    "# first a Dataset object is initialised using the training data and labels from tensor slices\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Using the Dataset.map function, the predefined preprocessing steps can be applied efficiently to each data entry\n",
    "dataset_train = dataset_train.map(normalize_img)\n",
    "dataset_train = dataset_train.map(one_hot_encoding)\n",
    "# The data set is cached to increase performance\n",
    "dataset_train = dataset_train.cache()\n",
    "# after each epoch, the data set is randomly shuffled to decrease overfitting\n",
    "dataset_train = dataset_train.shuffle(x_train.shape[0])\n",
    "# the data set is divided into batches of 64\n",
    "dataset_train = dataset_train.batch(64)\n",
    "# during the evaluation at the end of each epoch the next batch is already prefetched to increase performance\n",
    "dataset_train = dataset_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation and test data sets are prepared similarly but they don't have to be shuffled, since they're not used for weight updates\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "dataset_val = dataset_val.map(normalize_img)\n",
    "dataset_val = dataset_val.map(one_hot_encoding)\n",
    "dataset_val = dataset_val.batch(64)\n",
    "dataset_val = dataset_val.cache()\n",
    "dataset_val = dataset_val.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "dataset_test = dataset_test.map(normalize_img)\n",
    "dataset_test = dataset_test.map(one_hot_encoding)\n",
    "dataset_test = dataset_test.batch(64)\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a06f5-619c-41cb-ac2e-00fef2cc6f2f",
   "metadata": {},
   "source": [
    "## Building a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960992c2-0286-4aed-95c3-fc65bf31c5ec",
   "metadata": {},
   "source": [
    "An artifical neural network can be set up using keras.layers. Each layer output is used as input for the following layer hence generating some thing similar to multi-layer-perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d357170f-ce24-4d60-bb8f-d99b6d943317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input layer is defined by the expected input shape of the data.\n",
    "input_layer = tf.keras.layers.Input(shape=(x_width, x_height), name=\"input_layer\")\n",
    "# since we're using 2D data we'll reshape the input into a vector of size 28x28 = 784 dimensions\n",
    "flatten = tf.keras.layers.Flatten(name=\"flatten\")(input_layer)\n",
    "# The Dense layer corresponds to the layers used in the multi-layer-perceptron. Each neuron is connected to each input, hence the name\n",
    "# The number of artificial neurons in the layer is defined by the units-parameter, and for the activation function we choose the sigmoid\n",
    "hidden_layer = tf.keras.layers.Dense(units=32,\n",
    "                                    activation=\"sigmoid\",\n",
    "                                    name=\"hidden_layer\")(flatten)\n",
    "# Another Dense layer is used as output. It consists of 10 neurons with sigmoid activation. Each neuron represents an output class and yields a score from 0 to 1\n",
    "# The neuron with the highest score is selected as output class. The objective of the training is to get this result vector as close to the one-hot label vector as possible\n",
    "output_layer = tf.keras.layers.Dense(units=10,\n",
    "                                    activation=\"sigmoid\",\n",
    "                                    name=\"output_layer\")(hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d7cf0-d76d-4599-a2f6-b16d75955bb0",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab8d1e9-a484-4514-b592-5881deba5eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden_layer (Dense)        (None, 32)                25120     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A keras.Model can be defined by specifying the input and the output vectors of the neural network assembled above\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=[output_layer])\n",
    "# The model needs to be compiled for the training. The most important parameters for the compilation are\n",
    "# the optimizer, the loss function and the accuracy metric.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(0.2), # RMSprop of SGD wechsel \n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "# The Model.summary returns some helpful information concerning the model architecture and sizes\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70393f89-f056-48f7-a464-9ea4f975cdf4",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae6746-7c2b-4546-bcbc-9b320f2b214a",
   "metadata": {},
   "source": [
    "After the data pipeline is configured and the model ist constructed and compile, the training can be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34fe90-b354-4a86-993a-1cfdb2715464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "922/922 [==============================] - 3s 1ms/step - loss: 0.1009 - categorical_accuracy: 0.1000 - val_loss: 0.1000 - val_categorical_accuracy: 0.0931\n",
      "Epoch 2/10\n",
      "264/922 [=======>......................] - ETA: 0s - loss: 0.1000 - categorical_accuracy: 0.0987"
     ]
    }
   ],
   "source": [
    "# The training can be performed using the Model.fit method. Most importantly the train and \n",
    "# validation data sets are specified, and for example the maximum number of training epochs.\n",
    "# Further options will be explored later during the course\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    validation_data=dataset_val,\n",
    "    epochs=10\n",
    "    )\n",
    "# The model can be saved in several file formats after training for deployment or further training\n",
    "model.save(\"./first_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f5b5f-560d-44ba-9ab4-f04131be6d4f",
   "metadata": {},
   "source": [
    "## Analyzing the Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e17209-5bb7-42bc-94ad-ae2d88af65e6",
   "metadata": {},
   "source": [
    "Training and validation metrics are important to monitor the model performance during the training process. It can point towards several problems such as overfitting or instability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b8ae8-bd55-4942-89df-b49b73e4916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The history contains the metrics' progress along the training:\", history.history.keys())\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(history.epoch, history.history[\"loss\"], label=\"Training Loss\", c=\"#266662\", linestyle=\"--\")\n",
    "ax1.plot(history.epoch, history.history[\"val_loss\"], label=\"Valdation Loss\", c=\"#ED5654\", linestyle=\"--\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(history.epoch, history.history[\"categorical_accuracy\"], c=\"#266662\", label=\"Training Accuracy\")\n",
    "ax2.plot(history.epoch, history.history[\"val_categorical_accuracy\"], c=\"#ED5654\", label=\"Validation Accuracy\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "fig.legend(loc=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caea0d-0c94-48a2-9ab8-8fce5365556f",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56432b8d-ccd5-4895-bb28-1cbfacebe344",
   "metadata": {},
   "source": [
    "An important step in the life cycle of a model is the evaluation on a test set. Before the model is deployed, it is important to make sure it performs as expected on a given test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0da61-697d-4aa4-82d8-2b2b7299149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.evaluate(dataset_test)\n",
    "print(\"Test Loss: \", test_prediction[0])\n",
    "print(\"Test Accuracy: \", test_prediction[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd5e94-1c9d-4941-91f5-e23ec240b486",
   "metadata": {},
   "source": [
    "## Understanding the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc07c8-7b82-45e5-b6e6-b58430b4a4c8",
   "metadata": {},
   "source": [
    "To understand the model in a bit more detail, we'll take a look inside. To that end, an example batch is extracted from the training data and predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca24311-4f76-4a9c-aa26-79471e10f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = dataset_train.as_numpy_iterator()\n",
    "example_batch = example_data.next()[0]\n",
    "example_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d6da2-4e72-46b4-8670-e5feb63162e1",
   "metadata": {},
   "source": [
    "As described above, the model predicts a score for each of the ten output classes. The class with the highest output score (as calculated by a sigmoid function) can be extraced by an argmax function and is declared the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b100876-6ad5-40b9-aa94-c1be920c197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(example_batch)\n",
    "print(\"The prediction result looks the following:\\n\", prediction[0])\n",
    "\n",
    "plt.matshow(example_batch[0], cmap=\"gist_gray\")\n",
    "plt.title(f\"Class {np.argmax(prediction[0])} with sigmoid score of {np.max(prediction[0]):.2f}\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9703e1-c50b-4d87-b9d2-d213486f8f07",
   "metadata": {},
   "source": [
    "The model inference can be inspected more closely by adding each layers output to the model output for each inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca6c3e0-25df-4d8e-9e92-e65a92170ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all layer outputs are added to the model output\n",
    "multi_out = tf.keras.Model(inputs=model.input, outputs=[layer.output for layer in model.layers])\n",
    "# the predict call now returns all specified outputs\n",
    "all_outputs = multi_out.predict(example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4333a4-0350-4e01-a6b7-81be54758897",
   "metadata": {},
   "source": [
    "The Flatten layer transforms the 2D input image into a 1D vector. For now we can only deal with vectors as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bf287-2b59-436d-b380-80ef2353d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following entry in the output list is the the output after the Flatten layer transforming the 28x28 pixels into a 784 parameter vector.\n",
    "# Each prediction is performed in batches of 64, hence, the first dimension (all_outputs[1].shape[0]) of each output is number of data samples in each batch\n",
    "print(\"Shape after flattening layer:\", all_outputs[1].shape)\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.bar(range(all_outputs[1].shape[1]), all_outputs[1][0,:], color=\"#266662\")\n",
    "plt.savefig(\"flattened_in.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7ce02-43df-4cc5-8f41-d057f4cbe53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The hidden layer processes these 784 inputs and calculates 64 outputs from them\n",
    "hidden_out = all_outputs[2]\n",
    "print(\"Output Shape of hidden layer:\", hidden_out.shape)\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.bar(range(hidden_out.shape[1]), hidden_out[0,:], color=\"#266662\")\n",
    "plt.xlabel(\"Output Number\")\n",
    "plt.ylabel(\"Activation Strength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698ef76-8bd6-4a90-8a4a-ebe87f593ccf",
   "metadata": {},
   "source": [
    "## Feature Embedding with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e5096-1d0e-4997-80a7-3669449db625",
   "metadata": {},
   "source": [
    "Since it's difficut to develop an intuitiion for such high dimensional feature spaces, a representation of the first two principal components can help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d4b28-21e5-4850-b583-8512e9bcbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "example_batch_2D = pca.fit_transform(hidden_out)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0ca17-a304-4f70-b0b1-19eea0696f35",
   "metadata": {},
   "source": [
    "Each image can now be embedded into the two dimensional space. Indeed, the different input classes are projected onto different regions of the feature space. This is an indication that the hidden layer arranges the data reasonably for the classification afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f31989-24ac-4f10-9b99-747e108dcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "def imscatter(x, y, images, ax=None, zoom=1):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    digits = list()\n",
    "    for x0, y0, im in zip(x, y, images):\n",
    "        im = OffsetImage(im, zoom=zoom, cmap=\"gist_gray\")\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        digits.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return digits\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "imscatter(example_batch_2D[:, 0],\n",
    "          example_batch_2D[:, 1],\n",
    "          example_batch,\n",
    "          zoom=0.5,\n",
    "          ax=ax)\n",
    "ax.set_xlabel(\"1st Main Component\")\n",
    "ax.set_ylabel(\"2nd Main Component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac6dec-223c-4380-b82b-4ea456b10433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
