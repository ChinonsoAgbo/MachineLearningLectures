{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2555758e-3aa3-4e6c-b85b-f657b0177354",
   "metadata": {},
   "source": [
    "# Building the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d54cfa-e83d-42b0-bcde-48e9592a2003",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to set set up a basic Vision Transformer model from scratch using Pytorch in order to understand the basic principles and theoretical background. The tutorial combines concepts from https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c and https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51. This second of three notebooks deals with setting up a ViT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d509c-f9e1-489a-9a41-9313c67eb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Nice trick to import function and class definitions from other notebooks\n",
    "from ipynb.fs.defs._1_Vit_Preprocessing import VitPreprocessor\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6210320-2002-48f8-809d-c7a57e5d869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# INFO: The PyTorch uses a different tensor shape than Tensorflpow\n",
    "# Tensorflow: [batch_size, height, width, channels]\n",
    "# Pytorch: [batch_size, channels, height, width]\n",
    "train_loader = DataLoader(training_data, batch_size=32)\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "image_shape = x_sample.shape\n",
    "print (f\"Image shape is: {image_shape} (batch size, channels, height, width)\")\n",
    "plt.matshow(x_sample[0][0], cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68b190-9c08-46e3-8905-a13da3b6b474",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "Unlike Batch Normalization, Layer Normalization does not calculate mean and standard deviation across a batch but, as the name indicates, across all units across a layer. Let's consider for example a layer for our 64 dimensional embedded space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3766b8bf-3a3f-4df1-b69d-78a71a627136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a layer normalization in Pytorch\n",
    "ln = nn.LayerNorm(64)\n",
    "# generate a random number tensor of shape (batch size, number of patches, embedded dimension) as we expect it after the pre processing\n",
    "batch_input = torch.rand(32, 16, 64) * 42 + 4\n",
    "# Let's see what happens to a patch embedding after LayerNorm\n",
    "print(f\"0th input batch before LayerNorm: {batch_input[0, 0, :].mean()} +- {batch_input[0, 0, :].std()}\")\n",
    "\n",
    "normalized_input = ln(batch_input)\n",
    "print(f\"0th input batch after LayerNorm: {normalized_input[0, 0, :].mean()} +- {normalized_input[0, 0, :].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8a878-d8fd-448b-b966-913684f3665c",
   "metadata": {},
   "source": [
    "## Scaled dot-Product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b710927-8cd0-43fe-af36-43bfe5b84dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    # using the bmm method for the matrix multiplications since we're dealing with batches\n",
    "    # bmm -> batch matrix multiplication\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = key.size(-1) ** 0.5\n",
    "    activated = softmax(temp / scale, dim=-1)\n",
    "    return activated.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136c7d4-e2a9-4a9e-bf17-e898cb0ddde0",
   "metadata": {},
   "source": [
    "## Attention Head Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6707aff-20e0-453e-8d56-72f4ee51103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dimension, dimension)\n",
    "        self.k = nn.Linear(dimension, dimension)\n",
    "        self.v = nn.Linear(dimension, dimension)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return scaled_dot_product_attention(self.q(tokens), self.k(tokens), self.v(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c495e5-0e75-4d5d-bf8d-480d59bec507",
   "metadata": {},
   "source": [
    "## Expanding to Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c07fd-b2df-4bf5-8989-6aad4af3c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_dimension):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(hidden_dimension) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * hidden_dimension, hidden_dimension)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.linear(\n",
    "            torch.cat([head(tokens) for head in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61346471-40b2-4c35-98af-729bd9e28538",
   "metadata": {},
   "source": [
    "## Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01e30b-cec1-46f8-b4ea-941828cba5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitBlock(nn.Module):\n",
    "    def __init__(self, hidden_dimension, n_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.hidden_dimension)\n",
    "        self.mha = MultiHeadAttention(self.n_heads, self.hidden_dimension)\n",
    "        self.norm2 = nn.LayerNorm(self.hidden_dimension)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dimension, mlp_ratio * self.hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_ratio * self.hidden_dimension, self.hidden_dimension)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mha(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c51bcb-882d-4c27-9f3f-526ea61fff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VitBlock(hidden_dimension=8, n_heads=2)\n",
    "x = torch.randn(32, 50, 8)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56737520-ebf9-4496-800d-03ffe1f65132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVit(nn.Module):\n",
    "    def __init__(self, image_shape, classes, p_size, embedded_dimension, n_heads=2, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.image_shape = image_shape\n",
    "        self.classes = classes\n",
    "        self.p_size = p_size\n",
    "        self.embedded_dimension = embedded_dimension\n",
    "        self.n_heads = n_heads\n",
    "        self.n_blocks = n_blocks\n",
    "        \n",
    "        self.preprocessor = VitPreprocessor(\n",
    "            self.image_shape,\n",
    "            self.p_size,\n",
    "            self.embedded_dimension\n",
    "        )\n",
    "        \n",
    "        self.vit_blocks = nn.ModuleList(\n",
    "            [VitBlock(self.embedded_dimension, self.n_heads) for _ in range(self.n_blocks)]\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.embedded_dimension, self.classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        out = self.preprocessor(images)\n",
    "\n",
    "        for vit_block in self.vit_blocks:\n",
    "            out = vit_block(out)\n",
    "\n",
    "        class_tokens = out[:, 0]\n",
    "\n",
    "        result = self.mlp(class_tokens)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb287800-9c06-4ab1-b205-71c9881347a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyVit(x_sample.shape,\n",
    "              classes=10,\n",
    "              p_size=7,\n",
    "              embedded_dimension=32,\n",
    "              n_heads=4,\n",
    "              n_blocks=3)              \n",
    "print(model(x_sample).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d6129-f91f-4847-bc81-304cb1a3ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Number of trainable parameters {params}\")\n",
    "print(f\"List of modules: \\n {model.modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c08b17-aa88-49fd-82db-5356b0886ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
